import autogen
import streamlit as st

openai_api_key = st.session_state['OPENAI_API_KEY'] if 'OPENAI_API_KEY' in st.session_state else ""

# User inputs
radio_opt = ["gpt-4", "gpt-3.5-turbo"]
selected_opt = st.sidebar.radio(label="Choose model", options=radio_opt)
if radio_opt.index(selected_opt) == 0:
    config_list = [{'api_key': openai_api_key, 'model': 'gpt-4'}]
    print(config_list)
else:
    config_list = [{'api_key': openai_api_key, 'model': 'gpt-3.5-turbo'}]
    print(config_list)


print("models to use: ", [config_list[i]["model"] for i in range(len(config_list))])

# create an AssistantAgent named "assistant"
assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={
        "seed": 42,  # seed for caching and reproducibility
        "config_list": config_list,  # a list of OpenAI API configurations
        "temperature": 0,  # temperature for sampling
    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API
)
# create a UserProxyAgent instance named "user_proxy"
user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,  # set to True or image name like "python:3" to use docker
    },
)


st.title("AutoGen Demo")
# st.write("您可以在这里直接下单哦~")


with st.container():
    question = st.chat_input("What date is today? Compare the year-to-date gain for META and TESLA.")
    if question:
        # the assistant receives a message from the user_proxy, which contains the task description
        user_proxy.initiate_chat(
            assistant,
            message=question,
        )
        s = "\n".join(f"{dct['role']}==>\n {dct['content']}" for dct in list(user_proxy.chat_messages.values())[-1])
        st.write(s)

